{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DjedH_aaBX1Q"
   },
   "outputs": [],
   "source": [
    "# This pipeline is to be run in Vertex Workbench\n",
    "# Install Kubeflow Pipelines and GCP AI Platform\n",
    "!pip3 install kfp --user -q\n",
    "!pip3 install --upgrade google-cloud-aiplatform --user -q\n",
    "!pip3 install --upgrade google-cloud-pipeline-components --user -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ibtaNujdBoo0"
   },
   "outputs": [],
   "source": [
    "from datetime import date, datetime\n",
    "from typing import NamedTuple # for passing data between steps\n",
    "import google.cloud.aiplatform as aip\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "\n",
    "from google.cloud.aiplatform.models import Model\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import component, Input, Artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "8PxRJnfzCARg"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"iowa-steam\"\n",
    "BUCKET_NAME = \"iowa-steam-source-data\"\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "TRAINING_DATA_URI = f\"{BUCKET_URI}/train_canonical.csv\"\n",
    "TEST_DATA_URI = f\"{BUCKET_URI}/test_held_out.csv\"\n",
    "PIPELINE_ROOT = f\"{BUCKET_URI}/pipeline_root/control\"\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y_%m_%d__%H_%M_%S\")\n",
    "\n",
    "TEST_BATCH_OUTPUT_FOLDER = f\"automl_batch_{TIMESTAMP}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "4kI4ogSRCSOy"
   },
   "outputs": [],
   "source": [
    "aip.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "mizGsOpks9h4"
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"pandas\",\n",
    "        \"fsspec\",\n",
    "        \"gcsfs\",\n",
    "        \"google-cloud-storage\",\n",
    "        \"google-cloud-aiplatform\"\n",
    "    ]\n",
    ")\n",
    "def preprocess_data_op(bucket_name: str, train_data_uri: str) -> NamedTuple(\n",
    "    'Outputs',\n",
    "    [('train_path', str), ('test_path', str)]\n",
    "):\n",
    "    # The following code is from Andre's preprocessing pipeline,\n",
    "    # modified to use GCP cloud storage\n",
    "    from google.cloud import storage\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "\n",
    "    TIMESTAMP = datetime.now().strftime(\"%Y_%m_%d__%H_%M_%S\")\n",
    "    \n",
    "    def process_data(data, content):\n",
    "    \n",
    "        '''\n",
    "        data is in a csv format\n",
    "\n",
    "        content is the column in data set that you want to process\n",
    "\n",
    "        Content_Parsed_4 is the final processed output \n",
    "\n",
    "        '''\n",
    "\n",
    "        #\\r and \\n\n",
    "        data['Content_Parsed_1'] = content.str.replace(\"\\r\", \" \")\n",
    "        data['Content_Parsed_1'] = data['Content_Parsed_1'].str.replace(\"\\n\", \" \")\n",
    "        data['Content_Parsed_1'] = data['Content_Parsed_1'].str.replace(\"    \", \" \")\n",
    "\n",
    "        # quotation marks\n",
    "        data['Content_Parsed_1'] = data['Content_Parsed_1'].str.replace('\"', '')\n",
    "\n",
    "        # Lower casing all words so that upper case words (ex: at the beginning of a sentence) \n",
    "        # are read the same as lower case words\n",
    "        data['Content_Parsed_2'] = data['Content_Parsed_1'].str.lower()\n",
    "\n",
    "        # punctuation signs\n",
    "        punctuation_signs = list(\"?:!.,;\")\n",
    "        data['Content_Parsed_3'] = data['Content_Parsed_2']\n",
    "\n",
    "        for i in punctuation_signs:\n",
    "            data['Content_Parsed_3'] = data['Content_Parsed_3'].str.replace(i, '')\n",
    "\n",
    "        # Possessive pronouns \n",
    "        data['Content_Parsed_4'] = data['Content_Parsed_3'].str.replace(\"'s\", \"\")\n",
    "\n",
    "        return data\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # read raw source data from GCS\n",
    "    data = pd.read_csv(train_data_uri)\n",
    "\n",
    "    # process data\n",
    "    processed = process_data(data, data['user_review'])\n",
    "\n",
    "    # Store as ephemeral CSV\n",
    "    processed_filename = f\"train_canonical_{TIMESTAMP}.csv\"\n",
    "    processed.to_csv(processed_filename, index=False, header=True)\n",
    "\n",
    "    # upload to GCS\n",
    "    gcs_dest_path = f\"data_processed/{processed_filename}\"\n",
    "    blob = bucket.blob(gcs_dest_path)\n",
    "    blob.upload_from_filename(processed_filename)\n",
    "\n",
    "    train_processed_path = f\"gs://{bucket_name}/{gcs_dest_path}\"\n",
    "    \n",
    "    # Process the test instances - split into individual files and create jsonl\n",
    "    test = pd.read_csv(TEST_DATA_URI)\n",
    "    test = preprocess_data(test, test['user_review'])\n",
    "                           \n",
    "    destination_prefix = TEST_BATCH_OUTPUT_FOLDER\n",
    "    file_subprefix = \"data\"    \n",
    "    input_file_blob_name = f\"{destination_prefix}/steam_reviews_batch_predict_test.jsonl\"\n",
    "    \n",
    "    batch_input_data = []\n",
    "    for i, r in test.iterrows():\n",
    "        # Structure each file as required by Vertex AI for batch prediction.\n",
    "        content = r['Content_Parsed_4']\n",
    "        blob_name = f\"{destination_prefix}/{file_subprefix}/{r['review_id']}.txt\"\n",
    "        uri = f\"{BUCKET_URI}/{blob_name}\"\n",
    "        instance = {\"content\": uri, \"mimeType\": \"text/plain\"}\n",
    "        batch_input_data.append(instance)\n",
    "    \n",
    "        # upload this to cloud storage\n",
    "        blob = bucket.blob(blob_name)\n",
    "        blob.upload_from_string(content)\n",
    "    \n",
    "    batch_string = '\\n'.join([str(d) for d in batch_input_data])\n",
    "    input_file_blob = bucket.blob(input_file_blob_name)\n",
    "    input_file_blob.upload_from_string(batch_string)\n",
    "    test_jsonl_path = f\"{BUCKET_URI}/{input_file_blob_name}\"\n",
    "    \n",
    "    return (train_processed_path, test_jsonl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "prY9_kEDCRst"
   },
   "outputs": [],
   "source": [
    "# Creates and returns jsonl files for both AutoML and AutoML Sentiment from the \n",
    "# preprocessed data.\n",
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"pandas\",\n",
    "        \"google-cloud-storage\",\n",
    "        \"fsspec\",\n",
    "        \"gcsfs\",\n",
    "    ])\n",
    "def create_automl_import_files_op(bucket_name: str, input_path: str) -> NamedTuple(\n",
    "    'Outputs',\n",
    "    [('classification_path', str), ('sentiment_path', str)]\n",
    "):\n",
    "    import pandas as pd\n",
    "    from google.cloud import storage\n",
    "    import json \n",
    "    from datetime import datetime\n",
    "    \n",
    "    TIMESTAMP = datetime.now().strftime(\"%Y_%m_%d__%H_%M_%S\")\n",
    "\n",
    "    # Converts from 0/1 to N/Y to improve readability\n",
    "    def convert_label(label, sent=False):\n",
    "        if label == 1:\n",
    "            if sent:\n",
    "                return 1\n",
    "            else:\n",
    "                return \"Y\"\n",
    "        else:\n",
    "            if sent:\n",
    "                return 0\n",
    "            else:\n",
    "                return \"N\"\n",
    "\n",
    "    # read csv\n",
    "    train = pd.read_csv(input_path)\n",
    "\n",
    "    # construct the json objects\n",
    "    content_col = \"Content_Parsed_4\"\n",
    "    suggestion_col = \"user_suggestion\"\n",
    "\n",
    "    json_items = []\n",
    "    for index, row in train.iterrows():\n",
    "        item = {\n",
    "            \"classificationAnnotation\": { \"displayName\": convert_label(row[suggestion_col]) },\n",
    "            \"textContent\": row[content_col]\n",
    "        }\n",
    "        json_items.append(item)\n",
    "\n",
    "    json_items_sent = []\n",
    "    for index, row in train.iterrows():\n",
    "        label = convert_label(row[suggestion_col], sent=True)\n",
    "        item = {\n",
    "            \"sentimentAnnotation\": {\"sentiment\": label, \"sentimentMax\": 1},\n",
    "            \"textContent\": row[content_col]\n",
    "        }\n",
    "        json_items_sent.append(item)\n",
    "\n",
    "    # Write jsonl import file for AutoML Classification\n",
    "    automl_import_filename = f\"automl_import_train_{TIMESTAMP}.jsonl\"\n",
    "    with open(automl_import_filename, \"w\") as outfile:\n",
    "        for item in json_items:\n",
    "            outfile.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "    # Write jsonl import file for AutoML Sentiment Analysis\n",
    "    automl_sent_import_filename = f\"automl_sent_import_train_{TIMESTAMP}.jsonl\"\n",
    "    with open(automl_sent_import_filename, \"w\") as outfile:\n",
    "        for item in json_items_sent:\n",
    "            outfile.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "    # Upload these files to GCS\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    prefix = \"import_files\"\n",
    "    aml_import = bucket.blob(f\"{prefix}/{automl_import_filename}\")\n",
    "    aml_sent_import = bucket.blob(f\"{prefix}/{automl_sent_import_filename}\")\n",
    "\n",
    "    aml_import.upload_from_filename(automl_import_filename)\n",
    "    aml_sent_import.upload_from_filename(automl_sent_import_filename)\n",
    "\n",
    "    gcs_aml = f\"gs://{bucket_name}/{prefix}/{automl_import_filename}\"\n",
    "    gcs_aml_sent = f\"gs://{bucket_name}/{prefix}/{automl_sent_import_filename}\"\n",
    "\n",
    "    return (gcs_aml, gcs_aml_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "rULS779GC33Y"
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"iowa-steam-sentiment-pipeline\",\n",
    "    pipeline_root=PIPELINE_ROOT\n",
    ")\n",
    "\n",
    "def pipeline():\n",
    "    preprocess_op = preprocess_data_op(BUCKET_NAME, TRAINING_DATA_URI)\n",
    "\n",
    "    import_op = create_automl_import_files_op(BUCKET_NAME, preprocess_op.outputs['train_path'])\n",
    "\n",
    "    text_ds_op = gcc_aip.TextDatasetCreateOp(\n",
    "        project=PROJECT_ID,\n",
    "        display_name=f\"iowa-steam-reviews-processed-{TIMESTAMP}\",\n",
    "        gcs_source=import_op.outputs['classification_path'],\n",
    "        import_schema_uri=aiplatform.schema.dataset.ioformat.text.single_label_classification\n",
    "    ).set_display_name(\"create-text-dataset\")\n",
    "    \n",
    "    sentiment_ds_op = gcc_aip.TextDatasetCreateOp(\n",
    "        project=PROJECT_ID,\n",
    "        display_name=f\"iowa-steam-reviews-processed-sentiment-{TIMESTAMP}\",\n",
    "        gcs_source=import_op.outputs['sentiment_path'],\n",
    "        import_schema_uri=aiplatform.schema.dataset.ioformat.text.sentiment\n",
    "    ).set_display_name(\"create-sentiment-dataset\")\n",
    "    \n",
    "    automl_class_training_op = gcc_aip.AutoMLTextTrainingJobRunOp(\n",
    "        dataset=text_ds_op.outputs[\"dataset\"],\n",
    "        display_name=f\"automl_classification_{TIMESTAMP}\",\n",
    "        prediction_type=\"classification\",\n",
    "        multi_label=False,\n",
    "        training_fraction_split=0.6,\n",
    "        validation_fraction_split=0.2,\n",
    "        test_fraction_split=0.2,\n",
    "        model_display_name=f\"automl_classifcation_{TIMESTAMP}\",\n",
    "        project=PROJECT_ID\n",
    "    ).set_display_name(\"train_automl_classification\").after(text_ds_op)\n",
    "    \n",
    "    automl_sentiment_training_op = gcc_aip.AutoMLTextTrainingJobRunOp(\n",
    "        dataset=sentiment_ds_op.outputs[\"dataset\"],\n",
    "        display_name=f\"automl_classification_{TIMESTAMP}\",\n",
    "        prediction_type=\"sentiment\",\n",
    "        sentiment_max=1,\n",
    "        multi_label=False,\n",
    "        training_fraction_split=0.6,\n",
    "        validation_fraction_split=0.2,\n",
    "        test_fraction_split=0.2,\n",
    "        model_display_name=f\"automl_classifcation_{TIMESTAMP}\",\n",
    "        project=PROJECT_ID\n",
    "    ).set_display_name(\"train_automl_sentiment\").after(sentiment_ds_op)\n",
    "    \n",
    "    automl_class_endpoint_op = gcc_aip.EndpointCreateOp(\n",
    "        project=PROJECT_ID,\n",
    "        display_name = \"automl_endpoint_classification\"\n",
    "    ).set_display_name(\"create_classification_endpoint\").after(automl_class_training_op)\n",
    "    \n",
    "    automl_sentiment_endpoint_op = gcc_aip.EndpointCreateOp(\n",
    "        project=PROJECT_ID,\n",
    "        display_name = \"automl_endpoint_sentiment\"\n",
    "    ).set_display_name(\"create_sentiment_endpoint\").after(automl_sentiment_training_op)\n",
    "    \n",
    "    automl_class_deploy_op = gcc_aip.ModelDeployOp(\n",
    "        model=automl_class_training_op.outputs[\"model\"],\n",
    "        endpoint=automl_class_endpoint_op.outputs['endpoint'],\n",
    "        automatic_resources_min_replica_count=1,\n",
    "        automatic_resources_max_replica_count=1,\n",
    "    ).set_display_name(\"deploy_classification_model\").after(automl_class_endpoint_op)\n",
    "    \n",
    "    automl_sentiment_deploy_op = gcc_aip.ModelDeployOp(\n",
    "        model=automl_sentiment_training_op.outputs[\"model\"],\n",
    "        endpoint=automl_sentiment_endpoint_op.outputs['endpoint'],\n",
    "        automatic_resources_min_replica_count=1,\n",
    "        automatic_resources_max_replica_count=1,\n",
    "    ).set_display_name(\"deploy_sentiment_model\").after(automl_sentiment_endpoint_op)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "kAU9bYbVIKIg"
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=\"iowa-steam-sentiment-pipeline.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "seccOsH6IUzE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/47224200977/locations/us-central1/pipelineJobs/iowa-steam-sentiment-pipeline-20221125205306\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/47224200977/locations/us-central1/pipelineJobs/iowa-steam-sentiment-pipeline-20221125205306')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/iowa-steam-sentiment-pipeline-20221125205306?project=47224200977\n"
     ]
    }
   ],
   "source": [
    "job = aip.PipelineJob(\n",
    "    display_name=\"iowa-steam-pipeline\",\n",
    "    template_path=\"iowa-steam-sentiment-pipeline.json\",\n",
    "    pipeline_root=PIPELINE_ROOT\n",
    ")\n",
    "\n",
    "job.submit(service_account=\"47224200977-compute@developer.gserviceaccount.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
