{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjedH_aaBX1Q"
      },
      "outputs": [],
      "source": [
        "# This pipeline is to be run in Vertex Workbench\n",
        "# Install Kubeflow Pipelines and GCP AI Platform\n",
        "!pip3 install kfp --user -q\n",
        "!pip3 install --upgrade google-cloud-platform --user -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import date, datetime\n",
        "from typing import NamedTuple # for passing data between steps\n",
        "import google.cloud.aiplatform as aip\n",
        "from google.cloud import aiplatform\n",
        "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
        "\n",
        "from kfp import dsl\n",
        "from kfp.v2 import compiler\n",
        "from kfp.v2.dsl import component"
      ],
      "metadata": {
        "id": "ibtaNujdBoo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = \"iowa-steam\"\n",
        "BUCKET_NAME = \"iowa-source-steam-data\"\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
        "TRAINING_DATA_URI = f\"{BUCKET_URI}/train_canonical.csv\"\n",
        "PIPELINE_ROOT = f\"{BUCKET_URI}/pipeline_root/control\"\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y_%m_%d__%H_%M_%S\")"
      ],
      "metadata": {
        "id": "8PxRJnfzCARg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aip.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
      ],
      "metadata": {
        "id": "4kI4ogSRCSOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@component(\n",
        "    packages_to_install=[\n",
        "        \"pandas\", \n",
        "        \"numpy\", \n",
        "        \"matplotlib\", \n",
        "        \"pickle\", \n",
        "        \"google-cloud-storage\"\n",
        "    ]\n",
        ")\n",
        "def preprocess_data_op(bucket_name: str, train_data_uri: str) -> str:\n",
        "  # The following code is from Andre's preprocessing pipeline,\n",
        "  # modified to use GCP cloud storage\n",
        "  from google.cloud import storage\n",
        "  import pandas as pd\n",
        "\n",
        "  def process_data(data, content):\n",
        "    \n",
        "    '''\n",
        "    data is in a csv format\n",
        "    \n",
        "    content is the column in data set that you want to process\n",
        "    \n",
        "    Content_Parsed_4 is the final processed output \n",
        "    \n",
        "    '''\n",
        "    \n",
        "    #\\r and \\n\n",
        "    data['Content_Parsed_1'] = content.str.replace(\"\\r\", \" \")\n",
        "    data['Content_Parsed_1'] = data['Content_Parsed_1'].str.replace(\"\\n\", \" \")\n",
        "    data['Content_Parsed_1'] = data['Content_Parsed_1'].str.replace(\"    \", \" \")\n",
        "\n",
        "    # quotation marks\n",
        "    data['Content_Parsed_1'] = data['Content_Parsed_1'].str.replace('\"', '')\n",
        "\n",
        "    # Lower casing all words so that upper case words (ex: at the beginning of a sentence) \n",
        "    # are read the same as lower case words\n",
        "    data['Content_Parsed_2'] = data['Content_Parsed_1'].str.lower()\n",
        "\n",
        "    # punctuation signs\n",
        "    punctuation_signs = list(\"?:!.,;\")\n",
        "    data['Content_Parsed_3'] = data['Content_Parsed_2']\n",
        "\n",
        "    for i in punctuation_signs:\n",
        "        data['Content_Parsed_3'] = data['Content_Parsed_3'].str.replace(i, '')\n",
        "    \n",
        "    # Possessive pronouns \n",
        "    data['Content_Parsed_4'] = data['Content_Parsed_3'].str.replace(\"'s\", \"\")\n",
        "    \n",
        "    return data\n",
        "\n",
        "  storage_client = storage.Client()\n",
        "  bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "  # read raw source data from GCS\n",
        "  data = pd.read_csv(train_data_uri)\n",
        "\n",
        "  # process data\n",
        "  processed = process_data(data)\n",
        "  \n",
        "  # Store as ephemeral CSV\n",
        "  processed_filename = f\"train_canonical_{TIMESTAMP}.csv\"\n",
        "  processed.to_csv(processed_filename, index=False, header=False)\n",
        "\n",
        "  # upload to GCS\n",
        "  gcs_dest_path = f\"data_processed/{processed_filename}\"\n",
        "  blob = bucket.blob(gcs_dest_path)\n",
        "  blob.upload_from_filename(processed_filename)\n",
        "\n",
        "  return gcs_dest_path"
      ],
      "metadata": {
        "id": "mizGsOpks9h4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-processing component\n",
        "# To do: integrate Andre's component here\n",
        "@component(\n",
        "    packages_to_install=[\n",
        "        \"pandas\",\n",
        "        \"google-cloud-storage\",\n",
        "        \"google-cloud-aiplatform\"\n",
        "    ])\n",
        "def create_automl_import_file_op(bucket_name: str) -> NamedTuple('Outputs', [('uri', str), ('size', int)]):\n",
        "  # Todo: integrate Andre's work\n",
        "  return (\"gs://test\", 1000)"
      ],
      "metadata": {
        "id": "prY9_kEDCRst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dsl.pipeline(\n",
        "    name=\"iowa-steam-sentiment-pipeline\",\n",
        "    pipeline_root=PIPELINE_ROOT\n",
        ")\n",
        "\n",
        "def pipeline():\n",
        "  import_op = create_automl_import_file_op(BUCKET_NAME)\n",
        "\n",
        "  ds_op = gcc_aip.TextDatasetCreateOp(\n",
        "      project=PROJECT_ID,\n",
        "      display_name=f\"iowa-steam-reviews-processed-{TIMESTAMP}\",\n",
        "      gcs_source=import_op.outputs['uri'],\n",
        "      import_schema_uri='aiplatform.schema.dataset.ioformat.text.single_label_classification',\n",
        "      sync=sync)\n",
        "  ds_op.wait()\n",
        "\n",
        "  # todo: chain together training of model"
      ],
      "metadata": {
        "id": "rULS779GC33Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compiler.Compiler.compile(\n",
        "    pipeline_func=pipeline,\n",
        "    package_path=\"iowa-steam-sentiment-pipeline.json\"\n",
        ")"
      ],
      "metadata": {
        "id": "kAU9bYbVIKIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "job = aip.PipelineJob(\n",
        "    display_name=\"iowa-steam-pipeline\",\n",
        "    template_path=\"iowa-steam-sentiment-pipeline.json\",\n",
        "    pipeline_root=PIPELINE_ROOT\n",
        ")\n",
        "\n",
        "job.submit(service_account=\"TODO: add service account from GCP\")"
      ],
      "metadata": {
        "id": "seccOsH6IUzE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}